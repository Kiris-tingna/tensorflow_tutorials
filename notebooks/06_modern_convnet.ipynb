{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tutorial on how to build a convnet modern changes, e.g. Batch Normalization, Leaky rectifiers, and strided convolution.\n",
      "forked from Parag K. Mital, Jan. 2016\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"Tutorial on how to build a convnet modern changes, e.g. Batch Normalization, Leaky rectifiers, and strided convolution.\n",
    "forked from Parag K. Mital, Jan. 2016\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_norm(x, phase_train, scope='bn', affine=True):\n",
    "    \"\"\"\n",
    "    Batch normalization on convolutional maps.\n",
    "    from: https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow\n",
    "    Only modified to infer shape from input tensor x.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x\n",
    "        Tensor, 4D batch * height * width * channel input maps\n",
    "    phase_train\n",
    "        boolean tf.Variable, true indicates training phase\n",
    "    scope\n",
    "        string, variable scope\n",
    "    affine\n",
    "        whether to affine-transform outputs\n",
    "    Return\n",
    "    ------\n",
    "    normed\n",
    "        batch-normalized maps\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        shape = x.get_shape().as_list()\n",
    "\n",
    "        beta = tf.Variable(tf.constant(0.0, shape=[shape[-1]]),\n",
    "                           name='beta', trainable=True)\n",
    "        gamma = tf.Variable(tf.constant(1.0, shape=[shape[-1]]),\n",
    "                            name='gamma', trainable=affine)\n",
    "\n",
    "        batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2], name='moments')\n",
    "        ema = tf.train.ExponentialMovingAverage(decay=0.9)\n",
    "        ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "        ema_mean, ema_var = ema.average(batch_mean), ema.average(batch_var)\n",
    "\n",
    "        def mean_var_with_update():\n",
    "            \"\"\"Summary\n",
    "            Returns\n",
    "            -------\n",
    "            name : TYPE\n",
    "                Description\n",
    "            \"\"\"\n",
    "            with tf.control_dependencies([ema_apply_op]):\n",
    "                return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "        mean, var = tf.cond(phase_train, mean_var_with_update, lambda: (ema_mean, ema_var))\n",
    "\n",
    "        normed = tf.nn.batch_norm_with_global_normalization(x, mean, var, beta, gamma, 1e-3, affine)\n",
    "    return normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "    \"\"\"Leaky rectifier.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : Tensor\n",
    "        The tensor to apply the nonlinearity to.\n",
    "    leak : float, optional\n",
    "        Leakage parameter.\n",
    "    name : str, optional\n",
    "        Variable scope to use.\n",
    "    Returns\n",
    "    -------\n",
    "    x : Tensor\n",
    "        Output of the nonlinearity.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        f1 = 0.5 * (1 + leak)\n",
    "        f2 = 0.5 * (1 - leak)\n",
    "        return f1 * x + f2 * abs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, n_filters, f_h=5, f_w=5,\n",
    "           stride_h=2, stride_w=2,\n",
    "           stddev=0.02,\n",
    "           activation=None,\n",
    "           bias=True,\n",
    "           padding='SAME',\n",
    "           name=\"Conv2D\"):\n",
    "    \"\"\"2D Convolution with options for kernel size, stride, and init deviation.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : Tensor\n",
    "        Input tensor to convolve.\n",
    "    n_filters : int\n",
    "        Number of filters to apply.\n",
    "    f_h : int, optional\n",
    "        Filter height.\n",
    "    f_w : int, optional\n",
    "        Filter  width.\n",
    "    stride_h : int, optional\n",
    "        Stride in rows.\n",
    "    stride_w : int, optional\n",
    "        Stride in cols.\n",
    "    stddev : float, optional\n",
    "        Initialization's standard deviation.\n",
    "    activation : arguments, optional\n",
    "        Function which applies a nonlinearity\n",
    "    padding : str, optional\n",
    "        'SAME' or 'VALID'\n",
    "    name : str, optional\n",
    "        Variable scope to use.\n",
    "    Returns\n",
    "    -------\n",
    "    x : Tensor\n",
    "        Convolved input.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable('w', [f_h, f_w, x.get_shape()[-1], n_filters], initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
    "        conv = tf.nn.conv2d(x, w, strides=[1, stride_h, stride_w, 1], padding=padding)\n",
    "        if bias:\n",
    "            b = tf.get_variable('b', [n_filters], initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
    "            conv = tf.nn.bias_add(conv, b)\n",
    "        if activation:\n",
    "            conv = activation(conv)\n",
    "        return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(x, n_units, scope=None, stddev=0.02, activation=lambda x: x):\n",
    "    \"\"\"Fully-connected network.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : Tensor\n",
    "        Input tensor to the network.\n",
    "    n_units : int\n",
    "        Number of units to connect to.\n",
    "    scope : str, optional\n",
    "        Variable scope to use.\n",
    "    stddev : float, optional\n",
    "        Initialization's standard deviation.\n",
    "    activation : arguments, optional\n",
    "        Function which applies a nonlinearity\n",
    "    Returns\n",
    "    -------\n",
    "    x : Tensor\n",
    "        Fully-connected output.\n",
    "    \"\"\"\n",
    "    shape = x.get_shape().as_list()\n",
    "\n",
    "    with tf.variable_scope(scope or \"Linear\"):\n",
    "        matrix = tf.get_variable(\"Matrix\", [shape[-1], n_units], tf.float32, tf.random_normal_initializer(stddev=stddev))\n",
    "        return activation(tf.matmul(x, matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MNIST(one_hot=True):\n",
    "    \"\"\"Returns the MNIST dataset.\n",
    "    Returns\n",
    "    -------\n",
    "    mnist : DataSet\n",
    "        DataSet object w/ convenienve props for accessing\n",
    "        train/validation/test sets and batches.\n",
    "    \"\"\"\n",
    "    return input_data.read_data_sets('MNIST_data/', one_hot=one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Setup input to the network and true output label.  These are\n",
    "# simply placeholders which we'll fill in later.\n",
    "mnist = MNIST()\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We add a new type of placeholder to denote when we are training.\n",
    "# This will be used to change the way we compute the network during\n",
    "# training/testing.\n",
    "is_training = tf.placeholder(tf.bool, name='is_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll convert our MNIST vector data to a 4-D tensor:\n",
    "# batch * width * height * channel\n",
    "x_tensor = tf.reshape(x, [-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll use a new method called batch normalization.\n",
    "# This process attempts to \"reduce internal covariate shift\"\n",
    "# which is a fancy way of saying that it will normalize updates for each batch \n",
    "# using a smoothed version of the batch mean and variance\n",
    "# The original paper proposes using this before any nonlinearities\n",
    "h_1 = lrelu(batch_norm(conv2d(x_tensor, 32, name='conv1'), is_training, scope='bn1'), name='lrelu1')\n",
    "h_2 = lrelu(batch_norm(conv2d(h_1, 64, name='conv2'), is_training, scope='bn2'), name='lrelu2')\n",
    "h_3 = lrelu(batch_norm(conv2d(h_2, 64, name='conv3'), is_training, scope='bn3'), name='lrelu3')\n",
    "h_3_flat = tf.reshape(h_3, [-1, 64 * 4 * 4])\n",
    "h_4 = linear(h_3_flat, 10)\n",
    "y_pred = tf.nn.softmax(h_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define loss/eval/training functions\n",
    "cross_entropy = -tf.reduce_sum(y * tf.log(y_pred))\n",
    "train_step = tf.train.AdamOptimizer().minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We now create a new session to actually perform the initialization the\n",
    "# variables:\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9758\n",
      "0.9848\n",
      "0.9866\n",
      "0.985\n",
      "0.9896\n",
      "0.9866\n",
      "0.9878\n",
      "0.9848\n",
      "0.9886\n",
      "0.9898\n"
     ]
    }
   ],
   "source": [
    "# We'll train in minibatches and report accuracy:\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "for epoch_i in range(n_epochs):\n",
    "    for batch_i in range(mnist.train.num_examples // batch_size):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        sess.run(train_step, feed_dict={x: batch_xs, \n",
    "                                        y: batch_ys, \n",
    "                                        is_training: True\n",
    "                                       })\n",
    "    print(sess.run(accuracy, feed_dict={x: mnist.validation.images, \n",
    "                                        y: mnist.validation.labels, \n",
    "                                        is_training: False\n",
    "                                       }))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
